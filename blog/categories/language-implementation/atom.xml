<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Language Implementation | Robert Mosolgo]]></title>
  <link href="http://rmosolgo.github.io/blog/categories/language-implementation/atom.xml" rel="self"/>
  <link href="http://rmosolgo.github.io/"/>
  <updated>2020-07-27T16:05:47-04:00</updated>
  <id>http://rmosolgo.github.io/</id>
  <author>
    <name><![CDATA[Robert Mosolgo]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Trampolining]]></title>
    <link href="http://rmosolgo.github.io/blog/2018/09/23/trampolining/"/>
    <updated>2018-09-23T21:04:00-04:00</updated>
    <id>http://rmosolgo.github.io/blog/2018/09/23/trampolining</id>
    <content type="html"><![CDATA[<p>As part of my work on <a href="https://github.com/rmosolgo/graphql-ruby/pull/1394">improving GraphQL-Ruby&rsquo;s runtime performance</a>, I&rsquo;ve been reading <a href="http://www.eopl3.com/"><em>Essentials of Programming Languages</em></a>. Here, I try to apply their lesson about &ldquo;trampolining&rdquo;.</p>

<!-- more -->


<p><strong>TL;DR:</strong> I applied a thing I read in a textbook and it:</p>

<ul>
<li>reduced the stack trace size by 80%</li>
<li>reduced the live object count by 15%</li>
<li>kept the same runtime speed</li>
</ul>


<p>You can see the diff and benchmark results here: <a href="https://github.com/rmosolgo/graphql-ruby/compare/1b306fad...eef73b1">https://github.com/rmosolgo/graphql-ruby/compare/1b306fad...eef73b1</a></p>

<h2>The Problem</h2>

<p>It&rsquo;s a bit funny, but it&rsquo;s not <em>totally clear</em> to me what the book is trying to get at here. In the book, they talk about <em>control context</em> or <em>continuations</em> in a way that I would talk about &ldquo;stack frames&rdquo;. I think the problem is this: when you implement a programming language as an interpreter, you end up with recursive method calls, and that recursion builds up a big stack in the host language. This is bad because it hogs memory.</p>

<p>I can definitely <em>imagine</em> that this is a problem in Ruby, although I haven&rsquo;t measured it. GraphQL-Ruby uses recursion to execute GraphQL queries, and I can <em>imagine</em> that those recursive backtrace frames hog memory for a couple reasons:</p>

<ul>
<li>The control frames themselves (managed by YARV or something) take up memory in their own right</li>
<li>The control frames each have a lexical scope (<code>binding</code>), which, since it&rsquo;s still on the stack, can&rsquo;t be GCed. So, Ruby holds on to a lot of objects which <em>could</em> be garbaged collected if the library was written better.</li>
</ul>


<p>Besides that, the long backtrace adds a lot of noise when debugging.</p>

<h2>Trampolining</h2>

<p>In the book, they say, &ldquo;move your recursive calls to tail position, then, assuming your language has tail-call optimization, you won&rsquo;t have this problem.&rdquo; Well, my language <em>doesn&rsquo;t</em> have tail-call optimization, so I <em>do</em> have this problem! (Ok, it&rsquo;s an <a href="https://ruby-doc.org/core-2.4.0/RubyVM/InstructionSequence.html#method-c-compile_option-3D">option</a>.)</p>

<p>Luckily for me, they describe a technique for solving the problem <em>without</em> tail-call optimization. It&rsquo;s called <em>trampolining</em>, and it works roughly like this:</p>

<blockquote><p>When a method <em>would</em> make a recursive call, instead, return a <code>Bounce</code>. Then, the top-level method, which previously received the <code>FinalValue</code> of the interpreter&rsquo;s work, should be extended to accept <em>either</em> a <code>FinalValue</code> or a <code>Bounce</code>. In the case of a <code>FinalValue</code>, it returns the value as previously. In the case of a <code>Bounce</code>, it re-enters the interpreter using the &ldquo;bounced&rdquo; value.</p></blockquote>

<p>Using this technique, a previously-recursive method now <em>returns</em>, giving the caller some information about how to take the next step.</p>

<p>Let&rsquo;s give it a try.</p>

<h2>The Setup</h2>

<p>I want to test impact in two ways: memory consumption and backtrace size. I want to measure these values <em>during</em> GraphQL execution, so what better way to do it but build a GraphQL schema!</p>

<p>You can see the <a href="https://github.com/rmosolgo/graphql-ruby/compare/1b306fad...eef73b1#diff-7a29575d7b0f8a35812f9323ee46febe">whole benchmark</a>, but in short, we&rsquo;ll run a deeply-nested query, and at the deepest point, measure the <em>backtrace size</em> and the number of live objects in the heap:</p>

<pre><code class="ruby">{
  nestedMetric {
    nestedMetric {
      nestedMetric {
        # ... more nesting ...
        nestedMetric {
          backtraceSize
          objectCount
        }
      }
    }
  }
}
</code></pre>

<p>Where the fields are implemented by:</p>

<pre><code class="ruby">def backtrace_size
  caller.size
end

def object_count
  # Make a GC pass
  GC.start
  # Count how many objects are alive in the heap,
  # subtracting the number of live objects before we started
  GC.stat[:heap_live_slots] - self.class.object_count_baseline
end
</code></pre>

<p>We&rsquo;ll use these measurements to assess the impact of the refactor.</p>

<h2>The Pledge: Recursive calls</h2>

<p>To begin with, the interpreter is implemented as a set of recursive methods. The methods do things like:</p>

<ul>
<li>Given an object and a set of selections, resolve the selected fields on that object</li>
<li>Given a value and a type, prepare the value for a GraphQL response according to the type</li>
</ul>


<p>These methods are <em>recursive</em> in the case of fields that return GraphQL objects. The first method resolves a field and calls the second method; then the second method, in order to prepare an object as a GraphQL response, calls back to the first method, to resolve selections on that object. For example, execution might work like this:</p>

<ul>
<li>Resolve selections on the root object

<ul>
<li>One of the selections returned a User

<ul>
<li>Resolve selections on the User

<ul>
<li>One of the selections returns a Repository

<ul>
<li>Resolve selections on the Repository

<ul>
<li>&hellip;</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>Do you see how the same procedure is being applied over and over, in a nested way? That&rsquo;s implemented with recursive calls in GraphQL-Ruby.</p>

<p>We can run our test to see how the Ruby execution context looks in this case:</p>

<pre><code class="ruby"># $ ruby test.rb
1b306fad3b6b35dd06248028883cd8a3ec4bdefd
{"backtraceSize"=&gt;282, "objectCount"=&gt;812}
</code></pre>

<p>This is the baseline for backtrace size and object count, which we&rsquo;re using to measure <em>memory overhead</em> in GraphQL execution. (This describes behavior at <a href="https://github.com/rmosolgo/graphql-ruby/commit/2401afc4a19f2e5616e1e155f953ec403bf4896c">this commit</a>.)</p>

<h2>The Turn: Moving Recursive Calls into Tail Position</h2>

<p>As a requirement for the final refactor, we have to do some code reorganization. In the current code, the recursive calls require some setup and teardown around them. For example, we track the GraphQL &ldquo;path&rdquo;, which is the list of fields that describe where we are in the response. Here&rsquo;s a field with its &ldquo;path&rdquo;:</p>

<pre><code class="ruby">{
  a {
    b {
      c # The path of this field ["a", "b", "c"]
    }
  }
}
</code></pre>

<p>In the code, it looks something like this:</p>

<pre><code class="ruby"># Append to the path for the duration of the nested call
@path.push(field_name)
# Continue executing, with the new path in context
execute_recursively(...)
# Remove the entry from `path`, since we're done here
@path.pop
</code></pre>

<p>The problem is, if I want to refactor <code>execute_recursively</code> to become a <code>Bounce</code>, it won&rsquo;t do me any good, because the value of <code>execute_recursively</code> <em>isn&rsquo;t returned</em> from the method. It&rsquo;s not the last call in the method, so its value isn&rsquo;t returned. Instead, the value of <code>@path.pop</code> is returned. (It&rsquo;s not used for anything.)</p>

<p>This is to say: <code>@path.pop</code> is in <em>tail position</em>, the last call in the method. But I want <code>execute_recursively</code> to be in tail position.</p>

<h3>A Hack Won&rsquo;t Work</h3>

<p>The easiest way to &ldquo;fix&rdquo; that would be to refactor the method to return the value of <code>execute_recursively</code>:</p>

<pre><code class="ruby"># Append to the path for the duration of the nested call
@path.push(field_name)
# Continue executing
return_value = execute_recursively(...)
# Remove the entry from `path`, since we're done here
@path.pop
# Manually return the execution value
return_value
</code></pre>

<p>The problem is, when <code>execute_recursively</code> is refactored to be a <code>Bounce</code>:</p>

<pre><code class="ruby"># Append to the path for the duration of the nested call
@path.push(field_name)
# Continue executing
bounce = prepare_bounce(...)
# Remove the entry from `path`, since we're done here
@path.pop
# Manually return the execution value
bounce
</code></pre>

<p>By the time the <code>bounce</code> is actually executed, <code>path</code> <em>won&rsquo;t have</em> the changes I need in it. The value is pushed <em>and popped</em> before the bounce is actually called.</p>

<h3>Pass the Path as Input</h3>

<p>The solution is to remove the need for <code>@path.pop</code>. This can be done by creating a <em>new path</em> and passing it as input.</p>

<pre><code class="ruby"># Create a new path for nested execution
new_path = path + [field_name]
# Pass it as an input
execute_recursively(new_path, ...)
</code></pre>

<p>Now, <code>execute_recursively</code> is in tail position!</p>

<p>(The actual refactor is here: <a href="https://github.com/rmosolgo/graphql-ruby/commit/ef6e94283ecf280b14fe5417a4ee6896a06ebe69">https://github.com/rmosolgo/graphql-ruby/commit/ef6e94283ecf280b14fe5417a4ee6896a06ebe69</a>)</p>

<h2>The Prestige: Make it Bounce</h2>

<p>Now, we want to replace recursive calls with a <em>bounce</em>, where a bounce is an object with enough information to continue execution at a later point in time.</p>

<p>Since my recursive interpreter is implemented with a bunch of stateless methods (they&rsquo;re stateless since the refactor above), I can create a Bounce class that will continue by calling the same method:</p>

<pre><code class="ruby">class Bounce
  # Take the inputs required to call the next method
  def initialize(object, method, *arguments)
    @object = object
    @method = method
    @arguments = arguments
  end

  # Continue by calling the method with the given inputs
  def continue
    @object.send(@method, *@arguments)
  end
end
</code></pre>

<p>Then, I replace the tail-position recursive calls with bounces:</p>

<pre><code class="diff">- execute_recursively(...)
+ Bounce.new(self, :execute_recursively, ...)
</code></pre>

<p>Instead of <em>growing</em> the backtrace by calling another method, we&rsquo;ll be <em>shrinking</em> the backtrace by returning from the current method with a Bounce.</p>

<p>You can see the refactor here: <a href="https://github.com/rmosolgo/graphql-ruby/commit/b8e51573652b736d67235080e8b450d6fc9cc92e">https://github.com/rmosolgo/graphql-ruby/commit/b8e51573652b736d67235080e8b450d6fc9cc92e</a></p>

<h3>How&rsquo;d it work?</h3>

<p>Let&rsquo;s run the test:</p>

<pre><code class="ruby"># $ ruby test.rb
b8e51573652b736d67235080e8b450d6fc9cc92e
{"backtraceSize"=&gt;55, "objectCount"=&gt;686}
</code></pre>

<p>It&rsquo;s a success! The <code>backtraceSize</code> decreased from 282 to 55. The <code>objectCount</code> decreased from <code>812</code> to <code>686</code>.</p>

<h3>Implementation Considerations</h3>

<p><strong>&ldquo;Trampolining&rdquo;</strong> is the process of taking each bounce and continuing it. In my first implementation, <code>def trampoline</code> looked like this:</p>

<pre><code class="ruby"># Follow all the bounces until there aren't any left
def trampoline(bounce)
  case bounce
  when Bounce
    trampoline(bounce.continue)
  when Array
    bounce.each { |b| trampoline(b) }
  else
    # not a bounce, do nothing
  end
end
</code></pre>

<p>My test indicated no improvement in memory overhead, so I frustratedly called it quits. While brushing my teeth before bed, it hit me! I had unwittingly <em>re-introduced</em> recursive method calls. So, I hurried downstairs and reimplemented <code>def trampoline</code> to use a <code>while</code> loop and a buffer of bounces, an approach which didn&rsquo;t grow the Ruby execution context. Then the test result was much better.</p>

<p>Another consideration is the <em>overhead of Bounces</em> themselves. My first implementation creates a bounce before resolving each field. For very large responses, this will add a lot of overhead, especially when the field is a simple leaf value. This should be improved somehow.</p>

<h2>What about Speed?</h2>

<p>It turns out that visitors to the website don&rsquo;t care about backtrace size or Ruby heap size, they just care about waiting for webpages to load. Lucky for me, my benchmark includes some runtime measurements, and the results were basically the same:</p>

<pre><code class="text"># before
Calculating -------------------------------------
                         92.144  (±10.9%) i/s -    456.000  in   5.022617s
# after
Calculating -------------------------------------
                        113.529  (± 7.9%) i/s -    567.000  in   5.031847s
</code></pre>

<p>The runtime performance was very similar, almost within the margin of error. However, the consideration of Bounce overhead described above could cause <em>worse</em> performance in some cases.</p>

<h2>What&rsquo;s next?</h2>

<p>This code isn&rsquo;t <em>quite</em> ready for GraphQL-Ruby, but I think it&rsquo;s promising for a few reasons:</p>

<ul>
<li>The reduction of memory overhead and backtrace noise could pay off for very large, nested queries</li>
<li>I might be able to leverage bounces to give the caller more control over how GraphQL queries are executed. For example, at GitHub, we use GraphQL queries when rendering HTML pages. With some work, maybe we could alternate between bouncing GraphQL and rendering HTML, so we&rsquo;d get a better progressive rendering experience on the front end.</li>
</ul>


<p>However, one serious issue still needs to be addressed: what about the <code>Bounce</code>&rsquo;s <em>own</em> overhead? Allocating a new object for <em>every field execution</em> is already a performance issue in GraphQL-Ruby, and I&rsquo;m trying hard to remove it. So the implementation will need to be more subtle in that regard.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Summer Reading: Specializing Ruby]]></title>
    <link href="http://rmosolgo.github.io/blog/2016/08/06/summer-reading-specializing-ruby/"/>
    <updated>2016-08-06T11:10:00-04:00</updated>
    <id>http://rmosolgo.github.io/blog/2016/08/06/summer-reading-specializing-ruby</id>
    <content type="html"><![CDATA[<p><a href="http://chrisseaton.com/phd/"><em>Specialising Dynamic Techniques for Implementing the Ruby Programming Language</em></a> (&ldquo;Specializing Ruby&rdquo;) is approachable and enjoyable (despite being a PhD thesis 😝).</p>

<!-- more -->


<p><em>Specializing Ruby</em> describes <a href="https://twitter.com/ChrisGSeaton">Chris Seaton</a>&rsquo;s work on <a href="http://chrisseaton.com/rubytruffle/">JRuby+Truffle</a>. It seems to be aimed at an unfamiliar audience, so it&rsquo;s loaded with background information and careful explanations. Those were a big benefit to me! I&rsquo;ll describe a few things that I enjoyed the most:</p>

<ul>
<li>Introduction to Truffle and Graal</li>
<li>Optimizing Metaprogramming with Dispatch Chains</li>
<li>Zero-Overhead Debugging</li>
<li>Interpreting Native Extensions</li>
</ul>


<h2>Introduction to Truffle and Graal</h2>

<p>Seaton&rsquo;s work is built on top of two existing Java projects: <strong>Truffle</strong> and <strong>Graal</strong> (pronunciation: 😖❓).</p>

<p>Truffle is a <em>language implementation framework</em> for <em>self-optimizing AST interpreters</em>. This means:</p>

<ul>
<li>Truffle is for <em>implementing languages</em>. People have used Truffle to implement many languages, including Ruby, C, and Python.</li>
<li>Truffle languages are <em>AST interpreters</em>. A Truffle language parses its source code into a tree of nodes (the <em>abstract syntax tree</em>, AST), which represents the program. Then, it executes the program by traversing the tree, taking actions at each node.</li>
<li>Truffle languages can <em>self-optimize</em>. Nodes can observe their execution and replace themselves with optimized versions of themselves.</li>
</ul>


<p>Graal is a <em>dynamic compiler</em> for the JVM, written in Java. A few points about Graal:</p>

<ul>
<li>It&rsquo;s a just-in-time compiler, so it improves a program&rsquo;s performance while the program runs.</li>
<li>Graal is written in Java, which means it can expose its own APIs to other Java programs (like Truffle).</li>
<li>Graal includes a powerful system for <em>de-optimizing</em>. This is especially important for Ruby, since Ruby&rsquo;s metaprogramming constructs allow programs to define new behavior for themselves while running.</li>
</ul>


<p>Truffle has a &ldquo;Graal backend,&rdquo; which supports close cooperation between the two. Together, they make a great team for language implementation: Truffle provides a simple approach to language design and Graal offers a means to optimize all the way to machine code.</p>

<h2>Optimizing Metaprogramming with Dispatch Chains</h2>

<p>This is a novel optimization technique for Ruby, described in section 5.</p>

<p>Since Ruby is dynamic, method lookups must happen at runtime. In CRuby, call sites have <em>caches</em> which store the result of method lookups and may short-circuit the lookup next time the call happens.</p>

<pre><code class="ruby">some_object.some_method(arg1, arg2)
#          ^- here's the call site
#             the _actual_ method definition to use
#             depends on `some_object`'s class, which is unknown
#             until the program is actually running
</code></pre>

<p>One such cache is a <em>polymorphic inline cache</em>, which is roughly a map of <code>Class =&gt; method</code> pairs. When CRuby starts the call, it checks the cache for the current receiver&rsquo;s class. On a cache hit, it uses the cached method definition. On a cache miss, it looks up a definition and adds it to the cache.</p>

<p>The cache might look like this:</p>

<pre><code class="ruby">some_object.some_method(arg1, arg2)
# Cache:
#   - SomeObject =&gt; SomeObject#some_method
#   - SomeOtherObject =&gt; SomeOtherObject#method_missing
</code></pre>

<p>In some cases, CRuby declares bankruptcy. Dynamic method calls (<code>.send</code>) are not cached!</p>

<pre><code class="ruby">some_object.send(method_name, arg1, arg2)
#          ^- who knows what method to call!?!?
</code></pre>

<p>JRuby+Truffle&rsquo;s solution to this challenge is <em>dispatch chains.</em> Each call site (including <code>.send</code>) gets a dispatch chain, which is a like two-layer cache. First, it stores the <em>name</em> of the method. Then, it stores the <em>class</em> of the receiver. For a &ldquo;static&rdquo; method call, it looks like this:</p>

<pre><code class="ruby">some_object.some_method(arg1, arg2)
# - "some_method" =&gt;
#    - SomeObject =&gt; SomeObject#some_method
#    - SomeOtherObject =&gt; SomeOtherObject#method_missing
</code></pre>

<p>And for a dynamic method call, it caches <em>each</em> method name:</p>

<pre><code class="ruby">some_object.send(method_name, arg1, arg2)
# - "some_method" =&gt;
#    - SomeObject =&gt; SomeObject#some_method
#    - SomeOtherObject =&gt; SomeOtherObject#method_missing
# - "some_other_method" =&gt;
#    - SomeObject =&gt; SomeObject#some_other_method
</code></pre>

<p>In this respect, JRuby+Truffle treats <em>every</em> method call like a <code>.send(...)</code>. This cache is implemented with Truffle nodes, so it&rsquo;s optimized as much as the rest of the program.</p>

<p>I wonder if this kind of method cache could be implemented for CRuby!</p>

<h2>Zero-Overhead Debugging</h2>

<p>Debugging in JRuby+Truffle (described in section 6) is a tour de force for the Truffle-Graal combo. Other Rubies incur big performance penalties for debugging. Some require a special &ldquo;debug&rdquo; flag. But Seaton implements zero-overhead, always-available debugging by applying Truffle concepts in a new way.</p>

<p>Debugging hooks (such as the beginning of a new line) are added as &ldquo;transparent&rdquo; Truffle AST nodes, analogous to CRuby&rsquo;s <code>trace</code> instruction. By default, they don&rsquo;t do anything &ndash; they just call through to their child nodes. Since they&rsquo;re &ldquo;just&rdquo; Truffle nodes, they&rsquo;re optimized like the rest of the program (and since they&rsquo;re transparent, they&rsquo;re optimized away completely). When those nodes are targeted for debugging, they&rsquo;re de-optimized, updated with the appropriate debug code, and the program continues running (and self-optimizing). When the debugger is detached, the node de-optimizes again, replaces itself with transparent nodes again, and the program resumes.</p>

<p>This chapter included a good description of Graal&rsquo;s <code>Assumption</code> concept. Assumptions are attached to optimized code. As long as <code>isValid()</code> is true, optimized code is executed. However, when an assumption is marked as invalid, Graal transfers execution back to the interpreter. Debugging takes advantage of this construct: debug nodes are transparent under the assumption that no debugger is attached to them. But when a developer attaches a debugger, then that assumption is invalidated and Graal de-optimizes and starts interpreting with the new debug nodes. Removing a debugger does the same thing: it invalidates an assumption, automatically de-optimizing the compiled code.</p>

<h2>Interpreting Native Extensions</h2>

<p>Truffle: if it&rsquo;s not solving your problems, you&rsquo;re not using enough of it!</p>

<p>Throughout the paper, Seaton points out the &ldquo;real-world&rdquo; challenge of any new Ruby implementation: it simply <em>must</em> support <em>all</em> existing code, including C extensions! If you require developers to rewrite code for a new implementation, they probably won&rsquo;t bother with it.</p>

<p>He also points out that CRuby&rsquo;s C API is an implementer&rsquo;s nightmare (my words, not his). It&rsquo;s tightly coupled to CRuby&rsquo;s implementation it provides direct access to CRuby&rsquo;s memory (eg, string pointers).</p>

<p>Truffle&rsquo;s design offers a solution to this problem. Truffle languages implement common interfaces for AST nodes and objects, meaning that they can be <em>shared</em> between languages! With this technique, JRuby+Truffle can implement Ruby&rsquo;s C API by interpreting C with Truffle. Since it&rsquo;s &ldquo;just Truffle&rdquo;, C and Ruby ASTs can be seamlessly merged. They are even optimized together, just like a pure-Ruby program.</p>

<p>Seaton describes some particular techniques for adapting the pre-existing TruffleC project to the Ruby C API. In typical fashion, JRuby+Truffle outpaces CRuby &ndash; even for C extensions!</p>

<h2>Conclusion</h2>

<p>The only remaining question I have is, how bad is warm-up cost in practice? All of JRuby+Truffle&rsquo;s benchmarks are at &ldquo;peak performance&rdquo;, but the system is &ldquo;cold&rdquo; at start-up, and many triggers in the program can cause the system to de-optimize. Is JIT warm-up a real issue?</p>

<p>&ldquo;Optimizing Ruby&rdquo; was a great read. Although I found the subject matter quite challenging, the writing style and occasional illustrations helped me keep up. Practically speaking, I can&rsquo;t use JRuby+Truffle until it runs all of Ruby on Rails, which isn&rsquo;t the case <em>yet</em>. I&rsquo;m eager to see how this project matures!</p>
]]></content>
  </entry>
  
</feed>
